- 과제
1. 실습 



loss : 훈련 손실값
acc : 훈련정확도
val_loss : 검증 손실값
val_acc : 검증 정확도

relu : 은닉층으로 학습 'relu' 는 은닉층으로 역전파를 통해 
       좋은 성능이 나오기 때문에 마지막 층이 아니고서야 거의 relu 를 이용한다.
sigmond : yes or no 와 같은 이진 분류 문제
softmax : 확률 값을 이용해 다양한 클래스를 분류하기 위한 문제

MSE : 쓰는이유 : 오차의 음수값을 없애기 위해
        신경망의 출력 = x, 정답 = y 이라고 할 때 (x-y)^2/ 데이터갯수
batch_size : default 32   
epochs : default 100     

accuracy : 맞췄다 못맞췄다에 대한 값

실질적 값이 나와야하면 회귀로한다. 이럴땐 R2를 사용하여 정확도를 검사한다. 
회귀모델 -> 근사값이 나옴 -> accuracy를 사용할 수 없다.

분류모델 -> 한다 안한다 , 맞췄다 틀렸다 
            -> 결과가  0 or 1 -> accuracy사용 가능

Regression : 회귀 -> 근사값
Classification : 분류 -> 0 or 1

model.compile : 실제로 컴파일 할까 안할까.
metircs : 훈련의 평가지표만 보여준다 영향을 미치진 않는다.

compile구간에서 선언한 것들이 
model.evaluate 안에 리스트로 들어가있다 ex) loss, metrics
metrics를 compile구간에서 선언하지않으면 출력하지 않는다.
metrics는 리스트형태로 선언 및 출력이 가능하다.

model.evalueate는 compile에서 선언된 것을 리스트형태로 출력한다.

MAE(mean absolute error) : 평균절대오차 : |측정값 - 실제값|

총 데이터에서 훈련,평가 데이터를 나눈다.
훈련,평가 데이터는 서로 달라야한다.

# 회귀모델에서 검증에 사용되는 평가지표 : RMSE R2
# RMSE
from sklearn.metrics import mean_squared_error
def RMSE(y_test,y_predict):
        return np.sqrt(mean_squared_error(y_test,y_predict))
print("RMSE : ",RMSE(y_test,y_predict))

# R2
from sklearn.metrics import r2_score
r2 = r2_score(y_test,y_predict)
print("R2 : ",r2)


RMSE(Root mean absolute error) : mse에 루트를 씌운것
numpy sqrt : 배열 원소의 제곱근(Square Root) 범용 함수 
               : 리스트의 값들을 루트해준다.

R2(결정계수) : 회귀 모델에서 예측의 적합도를 0과 1사이의 값으로 계산한 것
                 : 회귀식의 정확도 평가

회귀모델에서 검증에 사용되는 평가지표 : RMSE R2

RMSE는 MSE에서 제곱된 값을 루트로 풀어주기 때문에 왜곡이 덜하다.

Train Data : 훈련용 데이터
Test Data : 테스트 데이터
Validation Data : 검증용 데이터

validation_data : fit()함수에서 사용된다.
                    : 검증용 데이터를 사용하여 훈련된 데이터와  비교한다.

validation_split : 따로 검증용데이터셋을 만들지않고 훈련용데이터에서 자동으로 자른다

# from sklearn.model_selection import train_test_split
# x_train , y_train , x_test , y_test = train_test_split(x , y , train_size=0.7)
 : 사이즈크기만 맞춰주면 알아서 잘라준다.
 : 0.7 이면 70%를 랜덤하게 뽑아낸다.
 : shuffle=false를 사용하면 순차적으로 잘라낸다.

- train_size : train자리(첫번째)에 있는 리스트의 크기를 결정한다.
- test_size : test자리(두번째)에 있는 리스트의 크기를 결정한다.


스칼라 : 개체 : 하나의 숫자    : 
                  x1
벡터 : 1차원 : 스칼라의 배열   
                 [x1, x2]
행렬(matrix) : 2차원 : 
                 [x1, x2]
                 [x3, x4]
텐서 : 3차원
               { [x1, x2]  [x5, x6] }
               { [x3, x4]  [x7, x8] }
 shape : 

MLP(다층 퍼셉트론) : Multi Layer Perceptron

numpy.array => 행렬화

배열을 인덱싱/슬라이싱 하는 함수들
ndarray[n, m] : n 행 m 열의 원소를 추출
ndarray[n, :] : n 행을 추출
ndarray[:, m] : m열을 추출
np.array.append : 배열을 추가할 수 있는 함수

range() : (x(부터) : y(전까지))
x[x:] : 리스트에서 x번째 다음부터 끝까지(인덱스)
x[:x] : 리스트에서 처음부터 x 전까지(인덱스) -> 인덱스x까지기 때문

optimizer adam : 

sqrt() : 숫자 x의 제곱근을 반환합니다. 루트씌운다


